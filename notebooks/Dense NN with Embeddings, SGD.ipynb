{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading and imports similar to other workbooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film size 1682.0\n",
      "user size 943.0\n"
     ]
    }
   ],
   "source": [
    "with open (\"../data/ml-100k/u.data\", \"r\") as f:\n",
    "    data = list(csv.reader(f, delimiter=\"\\t\"))\n",
    "data = np.array(data)\n",
    "film_dim = np.amax(np.array(data[:,1]).astype(np.float))\n",
    "user_dim = np.amax(np.array(data[:,0]).astype(np.float))\n",
    "print(\"film size\", film_dim)\n",
    "print(\"user size\", user_dim)\n",
    "data = data.astype(np.int)\n",
    "data = pd.DataFrame(data)\n",
    "data.sort_values([0,3],inplace=True) ## Sort data\n",
    "data.rename(columns= {0: 'user_id',\n",
    "                      1: 'movie_id',\n",
    "                      2: 'rating',\n",
    "                      3: 'time'},\n",
    "            inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the LabelEncoder to transform IDs using a relation (x,y) -> (0,n) where n = # of IDs, so that we do not have IDs just as a random number, but going from 0 to n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "data['user_id_encoded'] = user_encoder.fit_transform(data['user_id'].values)\n",
    "user_count = data['user_id_encoded'].nunique()\n",
    "\n",
    "movie_encoder = LabelEncoder()\n",
    "data['movie_id_encoded'] = movie_encoder.fit_transform(data['movie_id'].values)\n",
    "movie_count = data['movie_id_encoded'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data into test and train sets, but for better performance, we will rescale the ratings to a more Gaussian distribution as regression models tend to work better with normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_data = data[['user_id_encoded', 'movie_id_encoded']].values\n",
    "Y_data = data['rating'].values\n",
    "\n",
    "y_scaler = StandardScaler().fit(Y_data.reshape(len(Y_data), 1))\n",
    "y_scaled = y_scaler.transform(Y_data.reshape(len(Y_data), 1))[:, 0]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_data, y_scaled, test_size=0.1, random_state=315)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a number of factors for user / movie which we input into the Embedding layer. We also store the minimum and maximum ratings we need to use in the final Lambda layer and we prepare train and test arrays (each containing two arrays for users and for movies IDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Recommender model starting with an input layer followed by Embedding layers, one for both movies and users.  We then Concatenate the two Embeddings and follow them by a Dense layer with relu activation function. The final layers contains a Dense layer with a single numeric output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelDenseEmbed(x, y, f):\n",
    "    u = Input(shape=(1,))\n",
    "    m = Input(shape=(1,))\n",
    "    ue = Embedding(x, f)(u)\n",
    "    me = Embedding(y, f)(m)\n",
    "    x = Concatenate()([ue, me])\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('linear')(x)\n",
    "    model = Model(inputs=[user, movie], outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model and fit it to the training data. We experimented with a SGD but it turned out to train slower and with the same or worse final performance. We then resorted to Adam with 0.001 learning rate as it proved to be the most efficien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 64)        60352       input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 64)        107648      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 128)       0           embedding_8[0][0]                \n",
      "                                                                 embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1, 128)       0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1, 32)        4128        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1, 32)        0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1, 32)        0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1, 1)         33          dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1, 1)         0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 172,161\n",
      "Trainable params: 172,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "decay_rate = 0.01 / 50\n",
    "momentum = 0.5\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate)\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "model = ModelDenseEmbed(user_count, movie_count, 64)\n",
    "model.compile(loss='MSE', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "90000/90000 [==============================] - 14s 160us/sample - loss: 0.6092 - val_loss: 0.6630\n",
      "Epoch 2/5\n",
      "90000/90000 [==============================] - 14s 157us/sample - loss: 0.6091 - val_loss: 0.6676\n",
      "Epoch 3/5\n",
      "90000/90000 [==============================] - 13s 148us/sample - loss: 0.6089 - val_loss: 0.6651\n",
      "Epoch 4/5\n",
      "90000/90000 [==============================] - 14s 151us/sample - loss: 0.6060 - val_loss: 0.6675\n",
      "Epoch 5/5\n",
      "90000/90000 [==============================] - 13s 150us/sample - loss: 0.6062 - val_loss: 0.6647\n"
     ]
    }
   ],
   "source": [
    "training_res = model.fit(x=[train_x[:, 0], train_x[:, 1]], y=train_y, batch_size=64, epochs=50, verbose=1, validation_data=([test_x[:, 0], test_x[:, 1]], test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The desired models converges to a loss and val_loss value of abt. 0.66. The loss function used is mean squared error. Since the difference between the max and min of the y_scaled is 3.5, 0.66 can be considered a bit underwhelming result. We can print out some sample data to see the actual numbers (we need to inversely transform the data using the y_scaler instantiated sooner in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([test_x[:, 0], test_x[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"NNE_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"NNE_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('NNE_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"NNE_model.h5\")\n",
    "loaded_model.compile(loss='MSE', optimizer=adam)\n",
    "loaded_predictions = loaded_model.predict([test_x[:, 0], test_x[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.7539701]],\n",
       "\n",
       "       [[4.0057034]],\n",
       "\n",
       "       [[3.084084 ]],\n",
       "\n",
       "       [[3.8751   ]],\n",
       "\n",
       "       [[4.089307 ]],\n",
       "\n",
       "       [[3.6873972]],\n",
       "\n",
       "       [[4.6803074]],\n",
       "\n",
       "       [[5.0133653]],\n",
       "\n",
       "       [[4.27945  ]],\n",
       "\n",
       "       [[3.3583715]]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler.inverse_transform(loaded_predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.7539701]],\n",
       "\n",
       "       [[4.0057034]],\n",
       "\n",
       "       [[3.084084 ]],\n",
       "\n",
       "       [[3.8751   ]],\n",
       "\n",
       "       [[4.089307 ]],\n",
       "\n",
       "       [[3.6873972]],\n",
       "\n",
       "       [[4.6803074]],\n",
       "\n",
       "       [[5.0133653]],\n",
       "\n",
       "       [[4.27945  ]],\n",
       "\n",
       "       [[3.3583715]]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler.inverse_transform(predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 3., 4., 4., 5., 5., 5., 5., 5.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler.inverse_transform(test_y[10:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
