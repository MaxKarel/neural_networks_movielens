{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading and imports similar to other workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film size 1682.0\n",
      "user size 943.0\n"
     ]
    }
   ],
   "source": [
    "with open (\"../data/ml-100k/u.data\", \"r\") as f:\n",
    "    data = list(csv.reader(f, delimiter=\"\\t\"))\n",
    "data = np.array(data)\n",
    "film_dim = np.amax(np.array(data[:,1]).astype(np.float))\n",
    "user_dim = np.amax(np.array(data[:,0]).astype(np.float))\n",
    "print(\"film size\", film_dim)\n",
    "print(\"user size\", user_dim)\n",
    "data = data.astype(np.int)\n",
    "data = pd.DataFrame(data)\n",
    "data.sort_values([0,3],inplace=True) ## Sort data\n",
    "data.rename(columns= {0: 'user_id',\n",
    "                      1: 'movie_id',\n",
    "                      2: 'rating',\n",
    "                      3: 'time'},\n",
    "            inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the LabelEncoder to transform IDs using a relation (x,y) -> (0,n) where n = # of IDs, so that we do not have IDs just as a random number, but going from 0 to n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "data['user_id_encoded'] = user_encoder.fit_transform(data['user_id'].values)\n",
    "user_count = data['user_id_encoded'].nunique()\n",
    "\n",
    "movie_encoder = LabelEncoder()\n",
    "data['movie_id_encoded'] = movie_encoder.fit_transform(data['movie_id'].values)\n",
    "movie_count = data['movie_id_encoded'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data into test and train sets, but for better performance, we will rescale the ratings to a more Gaussian distribution as regression models tend to work better with normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_data = data[['user_id_encoded', 'movie_id_encoded']].values\n",
    "Y_data = data['rating'].values\n",
    "\n",
    "y_scaler = StandardScaler().fit(Y_data.reshape(len(Y_data), 1))\n",
    "y_scaled = y_scaler.transform(Y_data.reshape(len(Y_data), 1))[:, 0]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_data, y_scaled, test_size=0.1, random_state=315)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Recommender model starting with an input layer followed by Embedding layers, one for both movies and users.  We then Concatenate the two Embeddings and follow them by a Dense layer with relu activation function. We tried to add several othres but even with additional Dropout layers it had a tendency to overfit or oscilate (in each second epoch val_loss got worse and then went lower again), and the training was slower in general. The final layer is a Dense layer with a single numeric output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelDenseEmbed(x, y, f):\n",
    "    u = Input(shape=(1,))\n",
    "    m = Input(shape=(1,))\n",
    "    ue = Embedding(x, f)(u)\n",
    "    me = Embedding(y, f)(m)\n",
    "    x = Concatenate()([ue, me])\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('linear')(x)\n",
    "    model = Model(inputs=[u, m], outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model and fit it to the training data. We experimented with a SGD but it turned out to train slower and with the same or worse final performance. We then resorted to Adam with 0.001 learning rate as it proved to be the most efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, 1, 64)        60352       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 1, 64)        107648      input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 1, 128)       0           embedding_24[0][0]               \n",
      "                                                                 embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 1, 128)       0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1, 32)        4128        dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 1, 32)        0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 1, 32)        0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 1, 1)         33          dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 1, 1)         0           dense_34[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 172,161\n",
      "Trainable params: 172,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "decay_rate = 0.01 / 50\n",
    "momentum = 0.5\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate)\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "model = ModelDenseEmbed(user_count, movie_count, 64)\n",
    "model.compile(loss='MSE', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "90000/90000 [==============================] - 15s 162us/sample - loss: 0.6498 - val_loss: 0.6693\n",
      "Epoch 2/10\n",
      "90000/90000 [==============================] - 13s 148us/sample - loss: 0.6447 - val_loss: 0.6696\n",
      "Epoch 3/10\n",
      "90000/90000 [==============================] - 13s 150us/sample - loss: 0.6425 - val_loss: 0.6631\n",
      "Epoch 4/10\n",
      "90000/90000 [==============================] - 13s 149us/sample - loss: 0.6395 - val_loss: 0.6631\n",
      "Epoch 5/10\n",
      "90000/90000 [==============================] - 13s 149us/sample - loss: 0.6385 - val_loss: 0.6667\n",
      "Epoch 6/10\n",
      "90000/90000 [==============================] - 13s 150us/sample - loss: 0.6341 - val_loss: 0.6633\n",
      "Epoch 7/10\n",
      "90000/90000 [==============================] - 14s 158us/sample - loss: 0.6316 - val_loss: 0.6650\n",
      "Epoch 8/10\n",
      "90000/90000 [==============================] - 14s 155us/sample - loss: 0.6297 - val_loss: 0.6652\n",
      "Epoch 9/10\n",
      "90000/90000 [==============================] - 14s 158us/sample - loss: 0.6281 - val_loss: 0.6647\n",
      "Epoch 10/10\n",
      " 9344/90000 [==>...........................] - ETA: 12s - loss: 0.6058"
     ]
    }
   ],
   "source": [
    "csv_logger = CSVLogger('../logs/LSTM_log.csv', append=True, separator=';')\n",
    "training_res = model.fit(x=[train_x[:, 0], train_x[:, 1]], y=train_y, batch_size=64, epochs=10, verbose=1, validation_data=([test_x[:, 0], test_x[:, 1]], test_y, callbacks=[csv_logger]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The desired models converges to a loss and val_loss value of abt. 0.66. The loss function used is mean squared error. Since the difference between the max and min of the y_scaled is 3.5, 0.66 can be considered a bit underwhelming result. We can print out some sample data to see the actual numbers (we need to inversely transform the data using the y_scaler instantiated sooner in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([test_x[:, 0], test_x[:, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load the model and save it at will. We have stored a trained version produced by a 50 epoch run with the above configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"NNE_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"NNE_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('NNE_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"NNE_model.h5\")\n",
    "loaded_model.compile(loss='MSE', optimizer=adam)\n",
    "loaded_predictions = loaded_model.predict([test_x[:, 0], test_x[:, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to invert the scaling though to get actual star numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.7539701]],\n",
       "\n",
       "       [[4.0057034]],\n",
       "\n",
       "       [[3.084084 ]],\n",
       "\n",
       "       [[3.8751   ]],\n",
       "\n",
       "       [[4.089307 ]],\n",
       "\n",
       "       [[3.6873972]],\n",
       "\n",
       "       [[4.6803074]],\n",
       "\n",
       "       [[5.0133653]],\n",
       "\n",
       "       [[4.27945  ]],\n",
       "\n",
       "       [[3.3583715]]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler.inverse_transform(loaded_predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.7539701]],\n",
       "\n",
       "       [[4.0057034]],\n",
       "\n",
       "       [[3.084084 ]],\n",
       "\n",
       "       [[3.8751   ]],\n",
       "\n",
       "       [[4.089307 ]],\n",
       "\n",
       "       [[3.6873972]],\n",
       "\n",
       "       [[4.6803074]],\n",
       "\n",
       "       [[5.0133653]],\n",
       "\n",
       "       [[4.27945  ]],\n",
       "\n",
       "       [[3.3583715]]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler.inverse_transform(predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 3., 4., 4., 5., 5., 5., 5., 5.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler.inverse_transform(test_y[10:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
