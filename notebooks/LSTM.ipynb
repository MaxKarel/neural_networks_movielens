{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film size 1682.0\n",
      "user size 943.0\n"
     ]
    }
   ],
   "source": [
    "with open (\"../data/ml-100k/u.data\", \"r\") as f:\n",
    "    data = list(csv.reader(f, delimiter=\"\\t\"))\n",
    "data = np.array(data)\n",
    "film_dim = np.amax(np.array(data[:,1]).astype(np.float))\n",
    "user_dim = np.amax(np.array(data[:,0]).astype(np.float))\n",
    "print(\"film size\", film_dim)\n",
    "print(\"user size\", user_dim)\n",
    "data = data.astype(np.int)\n",
    "data = pd.DataFrame(data)\n",
    "data.sort_values([0,3],inplace=True) ## Sort data\n",
    "data.rename(columns= {0: 'user_id',\n",
    "                      1: 'movie_id',\n",
    "                      2: 'rating',\n",
    "                      3: 'time'},\n",
    "            inplace=True)\n",
    "data_x = []\n",
    "data_y = []\n",
    "max_len = 0\n",
    "for i in range(1, user_dim.astype(np.int)+1):\n",
    "    user = data[data.user_id == i]\n",
    "    if len(user) > max_len:\n",
    "        max_len = len(user)\n",
    "    data_x.append(user['movie_id'])\n",
    "    data_y.append(user['rating'])\n",
    "\n",
    "data_x =  tf.keras.preprocessing.sequence.pad_sequences(data_x,padding='post',maxlen=max_len)\n",
    "data_y =  tf.keras.preprocessing.sequence.pad_sequences(data_y,padding='post',maxlen=max_len)\n",
    "\n",
    "train_x = data_x[:700]\n",
    "test_x = data_x[700:]\n",
    "train_y = data_y[:700]\n",
    "test_y = data_y[700:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings size\n",
    "While LSTM units = 64 and movies were not order by time of rating\n",
    "\n",
    "| Hyperparamater-value | Accuracy | Test Accuracy |\n",
    "| -------------------- | -------- | ------------- |\n",
    "| 64                   | 46.94%   | 35.22%        |\n",
    "| 128                  | 50.09%   | 34.63%        |\n",
    "| 256                  | 53.71%   | 34.13%        |\n",
    "| 512                  | 58.82%   | 34.34%        |\n",
    "\n",
    "### LSTM Units\n",
    "While embedding size was 300 and movies were not order by time of rating\n",
    "\n",
    "| Hyperparamater-value | Accuracy | Test Accuracy |\n",
    "| -------------------- | -------- | ------------- |\n",
    "| lstm_units (32)      | 51.75%   | 33.73%        |\n",
    "| lstm_units (64)      | 53.68%   | 33.22%        |\n",
    "| lstm_units (128)     | 53.52%   | 33.76%        |\n",
    "\n",
    "### Sort data by time\n",
    "\n",
    "LSTM unit = 64\n",
    "Embeddings size = 512\n",
    "Accuracy: 63.40%\n",
    "Test Accuracy: 32.43%\n",
    "\n",
    "### Adding droput layer\n",
    "\n",
    "We have tried to add droput layer for our model to combat some overtraining in case there is some. But with parameters of LSTM units (64) and embbedings size (128) we didnt see any improvement:\n",
    "\n",
    "Accuracy was 51.64% and validadtion accuracy: was 34.92%.\n",
    "Therefore there are no significant improvements by adding Dropout(0.5) layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 243 samples\n",
      "Epoch 1/30\n",
      "700/700 [==============================] - 39s 55ms/sample - loss: 0.2291 - accuracy: 0.3323 - val_loss: 0.1864 - val_accuracy: 0.3680\n",
      "Epoch 2/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.2085 - accuracy: 0.3639 - val_loss: 0.1832 - val_accuracy: 0.3696\n",
      "Epoch 3/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.2044 - accuracy: 0.3775 - val_loss: 0.1820 - val_accuracy: 0.3813\n",
      "Epoch 4/30\n",
      "700/700 [==============================] - 29s 42ms/sample - loss: 0.2020 - accuracy: 0.3897 - val_loss: 0.1818 - val_accuracy: 0.3750\n",
      "Epoch 5/30\n",
      "700/700 [==============================] - 29s 42ms/sample - loss: 0.2002 - accuracy: 0.3941 - val_loss: 0.1822 - val_accuracy: 0.3746\n",
      "Epoch 6/30\n",
      "700/700 [==============================] - 30s 43ms/sample - loss: 0.1987 - accuracy: 0.3997 - val_loss: 0.1820 - val_accuracy: 0.3769\n",
      "Epoch 7/30\n",
      "700/700 [==============================] - 31s 44ms/sample - loss: 0.1972 - accuracy: 0.4061 - val_loss: 0.1843 - val_accuracy: 0.3661\n",
      "Epoch 8/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1966 - accuracy: 0.4084 - val_loss: 0.1861 - val_accuracy: 0.3578\n",
      "Epoch 9/30\n",
      "700/700 [==============================] - 30s 43ms/sample - loss: 0.1955 - accuracy: 0.4129 - val_loss: 0.1829 - val_accuracy: 0.3714\n",
      "Epoch 10/30\n",
      "700/700 [==============================] - 31s 44ms/sample - loss: 0.1934 - accuracy: 0.4202 - val_loss: 0.1840 - val_accuracy: 0.3710\n",
      "Epoch 11/30\n",
      "700/700 [==============================] - 32s 46ms/sample - loss: 0.1926 - accuracy: 0.4240 - val_loss: 0.1842 - val_accuracy: 0.3697\n",
      "Epoch 12/30\n",
      "700/700 [==============================] - 33s 48ms/sample - loss: 0.1908 - accuracy: 0.4296 - val_loss: 0.1848 - val_accuracy: 0.3666\n",
      "Epoch 13/30\n",
      "700/700 [==============================] - 33s 48ms/sample - loss: 0.1892 - accuracy: 0.4372 - val_loss: 0.1843 - val_accuracy: 0.3697\n",
      "Epoch 14/30\n",
      "700/700 [==============================] - 29s 42ms/sample - loss: 0.1863 - accuracy: 0.4459 - val_loss: 0.1869 - val_accuracy: 0.3649\n",
      "Epoch 15/30\n",
      "700/700 [==============================] - 31s 44ms/sample - loss: 0.1854 - accuracy: 0.4506 - val_loss: 0.1869 - val_accuracy: 0.3584\n",
      "Epoch 16/30\n",
      "700/700 [==============================] - 31s 44ms/sample - loss: 0.1847 - accuracy: 0.4560 - val_loss: 0.1892 - val_accuracy: 0.3565\n",
      "Epoch 17/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1817 - accuracy: 0.4662 - val_loss: 0.1942 - val_accuracy: 0.3420\n",
      "Epoch 18/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1801 - accuracy: 0.4703 - val_loss: 0.1924 - val_accuracy: 0.3555\n",
      "Epoch 19/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1790 - accuracy: 0.4741 - val_loss: 0.1920 - val_accuracy: 0.3552\n",
      "Epoch 20/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1772 - accuracy: 0.4802 - val_loss: 0.1939 - val_accuracy: 0.3517\n",
      "Epoch 21/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1759 - accuracy: 0.4854 - val_loss: 0.1938 - val_accuracy: 0.3552\n",
      "Epoch 22/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1752 - accuracy: 0.4893 - val_loss: 0.1977 - val_accuracy: 0.3398\n",
      "Epoch 23/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1736 - accuracy: 0.4933 - val_loss: 0.1985 - val_accuracy: 0.3399\n",
      "Epoch 24/30\n",
      "700/700 [==============================] - 30s 42ms/sample - loss: 0.1730 - accuracy: 0.4946 - val_loss: 0.1998 - val_accuracy: 0.3420\n",
      "Epoch 25/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1714 - accuracy: 0.5002 - val_loss: 0.1987 - val_accuracy: 0.3450\n",
      "Epoch 26/30\n",
      "700/700 [==============================] - 29s 42ms/sample - loss: 0.1707 - accuracy: 0.5023 - val_loss: 0.2024 - val_accuracy: 0.3370\n",
      "Epoch 27/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1703 - accuracy: 0.5031 - val_loss: 0.2014 - val_accuracy: 0.3412\n",
      "Epoch 28/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1689 - accuracy: 0.5090 - val_loss: 0.2032 - val_accuracy: 0.3401\n",
      "Epoch 29/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1681 - accuracy: 0.5121 - val_loss: 0.2030 - val_accuracy: 0.3355\n",
      "Epoch 30/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1675 - accuracy: 0.5145 - val_loss: 0.2021 - val_accuracy: 0.3391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f42a4971198>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class POSTagger(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(POSTagger, self).__init__()\n",
    "        lstm_units = 64\n",
    "        num_classes = 6\n",
    "        self.emb = Embedding(\n",
    "            input_dim=1683 + 1,\n",
    "            output_dim=128,\n",
    "            mask_zero=True,\n",
    "            trainable=True)\n",
    "\n",
    "        self.lstm = Bidirectional(LSTM(\n",
    "            units=lstm_units, return_sequences=True))\n",
    "        \n",
    "        self.dropout = Dropout(0.5)\n",
    "        \n",
    "        self.dense = Dense(\n",
    "            units=num_classes,\n",
    "            activation='softmax')\n",
    "        \n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        mask = self.emb.compute_mask(x)\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm(x, mask=mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = POSTagger()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "csv_logger = CSVLogger('../logs/LSTM_log.csv', append=True, separator=';')\n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=10,\n",
    "    epochs=30,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../models/LSTM\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since subclassed keras models canot be saved first we have to create and compile one and then we can load weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model = POSTagger()\n",
    "sample_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "sample_model.load_weights(\"../models/LSTM\")\n",
    "pred = sample_model.predict(test_x[0], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of prediction of our model\n",
    "\n",
    "we can see that for first user we are able to predict rating for each of this users movies. Since there is 5 star rating (+ 1 for not rated movie) we get array with prediction of 6 of those ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[269 286 690 300 313]\n",
      "[[[0.01448555 0.05996639 0.05869912 0.21839991 0.3288063  0.31964275]]\n",
      "\n",
      " [[0.03245169 0.10657497 0.14628832 0.17322175 0.26685464 0.27460858]]\n",
      "\n",
      " [[0.07072053 0.11664139 0.15948306 0.19422805 0.23257437 0.22635266]]\n",
      "\n",
      " [[0.0188613  0.0640035  0.11775716 0.26862097 0.31216374 0.21859339]]\n",
      "\n",
      " [[0.0063609  0.02451925 0.07248284 0.11742138 0.2614604  0.51775527]]]\n",
      "[5 4 4 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(test_x[0][:5])\n",
    "print(pred[:5])\n",
    "print(test_y[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the classification\n",
    "\n",
    "From predicted results we see that we are predicting label instead of numerical value. We tried to adjust this model for predicting continous values in range from (0,5>. We realised that using `sparse_categorical_crossentropy` is not the right choice and instead we tried with somethin more commonly used for regression like `mse`. Alas we were not able to adjust the layers of our model som out shape and dimensions would mathch with train_y.\n",
    "And other aproach lead to model which was prediciting values as if our vector of movie_ids was just list of x values for continuos function `f(x) = y` and all information about the movie and user preferences were lost. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
