{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film size 1682.0\n",
      "user size 943.0\n"
     ]
    }
   ],
   "source": [
    "with open (\"../data/ml-100k/u.data\", \"r\") as f:\n",
    "    data = list(csv.reader(f, delimiter=\"\\t\"))\n",
    "data = np.array(data)\n",
    "film_dim = np.amax(np.array(data[:,1]).astype(np.float))\n",
    "user_dim = np.amax(np.array(data[:,0]).astype(np.float))\n",
    "print(\"film size\", film_dim)\n",
    "print(\"user size\", user_dim)\n",
    "data = data.astype(np.int)\n",
    "data = pd.DataFrame(data)\n",
    "data.sort_values([0,3],inplace=True) ## Sort data\n",
    "data.rename(columns= {0: 'user_id',\n",
    "                      1: 'movie_id',\n",
    "                      2: 'rating',\n",
    "                      3: 'time'},\n",
    "            inplace=True)\n",
    "data_x = []\n",
    "data_y = []\n",
    "max_len = 0\n",
    "for i in range(1, user_dim.astype(np.int)+1):\n",
    "    user = data[data.user_id == i]\n",
    "    if len(user) > max_len:\n",
    "        max_len = len(user)\n",
    "    data_x.append(user['movie_id'])\n",
    "    data_y.append(user['rating'])\n",
    "\n",
    "data_x =  tf.keras.preprocessing.sequence.pad_sequences(data_x,padding='post',maxlen=max_len)\n",
    "data_y =  tf.keras.preprocessing.sequence.pad_sequences(data_y,padding='post',maxlen=max_len)\n",
    "\n",
    "train_x = data_x[:700]\n",
    "test_x = data_x[700:]\n",
    "train_y = data_y[:700]\n",
    "test_y = data_y[700:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings size\n",
    "While LSTM units = 64 and movies were not order by time of rating\n",
    "\n",
    "| Hyperparamater-value | Accuracy | Test Accuracy |\n",
    "| -------------------- | -------- | ------------- |\n",
    "| 64                   | 46.94%   | 35.22%        |\n",
    "| 128                  | 50.09%   | 34.63%        |\n",
    "| 256                  | 53.71%   | 34.13%        |\n",
    "| 512                  | 58.82%   | 34.34%        |\n",
    "\n",
    "### LSTM Units\n",
    "While embedding size was 300 and movies were not order by time of rating\n",
    "\n",
    "| Hyperparamater-value | Accuracy | Test Accuracy |\n",
    "| -------------------- | -------- | ------------- |\n",
    "| lstm_units (32)      | 51.75%   | 33.73%        |\n",
    "| lstm_units (64)      | 53.68%   | 33.22%        |\n",
    "| lstm_units (128)     | 53.52%   | 33.76%        |\n",
    "\n",
    "### Sort data by time\n",
    "\n",
    "LSTM unit = 64\n",
    "Embeddings size = 512\n",
    "Accuracy: 63.40%\n",
    "Test Accuracy: 32.43%\n",
    "\n",
    "### Adding droput layer\n",
    "\n",
    "We have tried to add droput layer for our model to combat some overtraining in case there is some. But with parameters of LSTM units (64) and embbedings size (128) we didnt see any improvement:\n",
    "\n",
    "Accuracy was 51.64% and validadtion accuracy: was 34.92%.\n",
    "Therefore there are no significant improvements by adding Dropout(0.5) layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 243 samples\n",
      "Epoch 1/30\n",
      "700/700 [==============================] - 37s 53ms/sample - loss: 0.2263 - accuracy: 0.3281 - val_loss: 0.1890 - val_accuracy: 0.3487\n",
      "Epoch 2/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.2089 - accuracy: 0.3615 - val_loss: 0.1828 - val_accuracy: 0.3743\n",
      "Epoch 3/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.2041 - accuracy: 0.3801 - val_loss: 0.1817 - val_accuracy: 0.3789\n",
      "Epoch 4/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.2022 - accuracy: 0.3874 - val_loss: 0.1818 - val_accuracy: 0.3777\n",
      "Epoch 5/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1996 - accuracy: 0.3976 - val_loss: 0.1814 - val_accuracy: 0.3779\n",
      "Epoch 6/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1977 - accuracy: 0.4038 - val_loss: 0.1843 - val_accuracy: 0.3679\n",
      "Epoch 7/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1971 - accuracy: 0.4090 - val_loss: 0.1821 - val_accuracy: 0.3732\n",
      "Epoch 8/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1952 - accuracy: 0.4145 - val_loss: 0.1835 - val_accuracy: 0.3711\n",
      "Epoch 9/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1940 - accuracy: 0.4186 - val_loss: 0.1823 - val_accuracy: 0.3720\n",
      "Epoch 10/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1920 - accuracy: 0.4235 - val_loss: 0.1838 - val_accuracy: 0.3697\n",
      "Epoch 11/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1895 - accuracy: 0.4350 - val_loss: 0.1842 - val_accuracy: 0.3659\n",
      "Epoch 12/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1882 - accuracy: 0.4389 - val_loss: 0.1852 - val_accuracy: 0.3605\n",
      "Epoch 13/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1864 - accuracy: 0.4482 - val_loss: 0.1872 - val_accuracy: 0.3562\n",
      "Epoch 14/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1840 - accuracy: 0.4560 - val_loss: 0.1864 - val_accuracy: 0.3649\n",
      "Epoch 15/30\n",
      "700/700 [==============================] - 30s 43ms/sample - loss: 0.1821 - accuracy: 0.4618 - val_loss: 0.1932 - val_accuracy: 0.3373\n",
      "Epoch 16/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1811 - accuracy: 0.4684 - val_loss: 0.1892 - val_accuracy: 0.3547\n",
      "Epoch 17/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1799 - accuracy: 0.4711 - val_loss: 0.1902 - val_accuracy: 0.3569\n",
      "Epoch 18/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1783 - accuracy: 0.4763 - val_loss: 0.1903 - val_accuracy: 0.3555\n",
      "Epoch 19/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1764 - accuracy: 0.4859 - val_loss: 0.1931 - val_accuracy: 0.3457\n",
      "Epoch 20/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1754 - accuracy: 0.4875 - val_loss: 0.1952 - val_accuracy: 0.3359\n",
      "Epoch 21/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1749 - accuracy: 0.4907 - val_loss: 0.1933 - val_accuracy: 0.3550\n",
      "Epoch 22/30\n",
      "700/700 [==============================] - 28s 40ms/sample - loss: 0.1739 - accuracy: 0.4903 - val_loss: 0.1971 - val_accuracy: 0.3502\n",
      "Epoch 23/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1730 - accuracy: 0.4950 - val_loss: 0.1950 - val_accuracy: 0.3545\n",
      "Epoch 24/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1715 - accuracy: 0.4990 - val_loss: 0.1971 - val_accuracy: 0.3519\n",
      "Epoch 25/30\n",
      "700/700 [==============================] - 28s 41ms/sample - loss: 0.1707 - accuracy: 0.5010 - val_loss: 0.2008 - val_accuracy: 0.3402\n",
      "Epoch 26/30\n",
      "700/700 [==============================] - 29s 42ms/sample - loss: 0.1697 - accuracy: 0.5044 - val_loss: 0.2017 - val_accuracy: 0.3400\n",
      "Epoch 27/30\n",
      "700/700 [==============================] - 30s 43ms/sample - loss: 0.1690 - accuracy: 0.5076 - val_loss: 0.2026 - val_accuracy: 0.3418\n",
      "Epoch 28/30\n",
      "700/700 [==============================] - 29s 41ms/sample - loss: 0.1678 - accuracy: 0.5106 - val_loss: 0.2040 - val_accuracy: 0.3358\n",
      "Epoch 29/30\n",
      "700/700 [==============================] - 29s 42ms/sample - loss: 0.1671 - accuracy: 0.5144 - val_loss: 0.2059 - val_accuracy: 0.3334\n",
      "Epoch 30/30\n",
      "700/700 [==============================] - 30s 43ms/sample - loss: 0.1662 - accuracy: 0.5164 - val_loss: 0.2045 - val_accuracy: 0.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb09e7747f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class POSTagger(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(POSTagger, self).__init__()\n",
    "        lstm_units = 64\n",
    "        num_classes = 6\n",
    "        self.emb = Embedding(\n",
    "            input_dim=1683 + 1,\n",
    "            output_dim=128,\n",
    "            mask_zero=True,\n",
    "            trainable=True)\n",
    "\n",
    "        self.lstm = Bidirectional(LSTM(\n",
    "            units=lstm_units, return_sequences=True))\n",
    "        \n",
    "        self.dropout = Dropout(0.5)\n",
    "        \n",
    "        self.dense = Dense(\n",
    "            units=num_classes,\n",
    "            activation='softmax')\n",
    "        \n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        mask = self.emb.compute_mask(x)\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm(x, mask=mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = POSTagger()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=10,\n",
    "    epochs=30,\n",
    "    validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../models/LSTM\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since subclassed keras models canot be saved first we have to create and compile one and then we can load weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model = POSTagger()\n",
    "sample_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "sample_model.load_weights(\"../models/LSTM\")\n",
    "pred = sample_model.predict(test_x[0], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of prediction of our model\n",
    "\n",
    "we can see that for first user we are able to predict rating for each of this users movies. Since there is 5 star rating (+ 1 for not rated movie) we get array with prediction of 6 of those ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01448555, 0.05996639, 0.05869912, 0.21839991, 0.3288063 ,\n",
       "         0.31964275]],\n",
       "\n",
       "       [[0.03245169, 0.10657497, 0.14628832, 0.17322175, 0.26685464,\n",
       "         0.27460858]],\n",
       "\n",
       "       [[0.07072053, 0.11664139, 0.15948306, 0.19422805, 0.23257437,\n",
       "         0.22635266]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.13320369, 0.15760657, 0.160003  , 0.18471362, 0.19620894,\n",
       "         0.16826418]],\n",
       "\n",
       "       [[0.13320369, 0.15760657, 0.160003  , 0.18471362, 0.19620894,\n",
       "         0.16826418]],\n",
       "\n",
       "       [[0.13320369, 0.15760657, 0.160003  , 0.18471362, 0.19620894,\n",
       "         0.16826418]]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
